{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./codellama-7b.Q4_0.gguf (version GGUF V2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-7b-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = codellama_codellama-7b-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.95 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.53 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '16384', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'codellama_codellama-7b-hf'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =    7758.42 ms\n",
      "llama_print_timings:      sample time =       1.50 ms /     9 runs   (    0.17 ms per token,  6004.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7758.30 ms /     5 tokens ( 1551.66 ms per token,     0.64 tokens per second)\n",
      "llama_print_timings:        eval time =   61424.98 ms /     8 runs   ( 7678.12 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time =   69209.88 ms /    13 tokens\n"
     ]
    }
   ],
   "source": [
    "# Instanciate the model\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from llama_cpp import Llama\n",
    "my_aweseome_llama_model = Llama(model_path=\"./codellama-7b.Q4_0.gguf\")\n",
    "\n",
    "\n",
    "prompt = \"This is a prompt\"\n",
    "max_tokens = 100\n",
    "temperature = 0.3\n",
    "top_p = 0.1\n",
    "echo = True\n",
    "stop = [\"Q\", \"\\n\"]\n",
    "\n",
    "\n",
    "# Define the parameters\n",
    "model_output = my_aweseome_llama_model(\n",
    "       prompt,\n",
    "       max_tokens=max_tokens,\n",
    "       temperature=temperature,\n",
    "       top_p=top_p,\n",
    "       echo=echo,\n",
    "       stop=stop,\n",
    "   )\n",
    "final_result = model_output[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/fatemehbalaei/Downloads/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/fatemehbalaei/Downloads/llama-2-7b-chat.Q4_0.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stephen Colbert:\n",
      "Yo, John, I heard you've been talking smack about me on your show.\n",
      "Let's settle this like men, in a rap battle!\n",
      "\n",
      "John Oliver:\n",
      "Ha! You think you can rhyme your way out of this one, Stephen?\n",
      "You're the king of late night, but when it comes to rapping, you're no Kendrick.\n",
      "I'll leave you in the dust like a used tissue.\n",
      "Stephen Colbert:\n",
      "Oh, is that the best you got? You think you can take me down with some lame rhymes?\n",
      "I'm the one who brought truth to comedy, not you, John.\n",
      "My flow's so smooth, it'll make your head spin like a dreidel.\n",
      "John Oliver:\n",
      "You may have the upper hand in satire, Stephen, but when it comes to rap, I'm the real deal!\n",
      "I'll leave you in the shadows with your puny rhymes and lame beats.\n",
      "Stephen Colbert:\n",
      "Bring it on, John! Let's see what you got!\n",
      "I'm not afraid of your little raps, my flow is too tight to be beat.\n",
      "John Oliver:\n",
      "You may have the confidence, Stephen, but when it comes to skill, I'm the one who reigns supreme!\n",
      "I'll leave you in the dust like a used tissue, and then some!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6658.54 ms\n",
      "llama_print_timings:      sample time =      55.01 ms /   322 runs   (    0.17 ms per token,  5853.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13568.64 ms /    16 tokens (  848.04 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:        eval time = 2267959.80 ms /   321 runs   ( 7065.30 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time = 2284831.90 ms /   337 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nStephen Colbert:\\nYo, John, I heard you've been talking smack about me on your show.\\nLet's settle this like men, in a rap battle!\\n\\nJohn Oliver:\\nHa! You think you can rhyme your way out of this one, Stephen?\\nYou're the king of late night, but when it comes to rapping, you're no Kendrick.\\nI'll leave you in the dust like a used tissue.\\nStephen Colbert:\\nOh, is that the best you got? You think you can take me down with some lame rhymes?\\nI'm the one who brought truth to comedy, not you, John.\\nMy flow's so smooth, it'll make your head spin like a dreidel.\\nJohn Oliver:\\nYou may have the upper hand in satire, Stephen, but when it comes to rap, I'm the real deal!\\nI'll leave you in the shadows with your puny rhymes and lame beats.\\nStephen Colbert:\\nBring it on, John! Let's see what you got!\\nI'm not afraid of your little raps, my flow is too tight to be beat.\\nJohn Oliver:\\nYou may have the confidence, Stephen, but when it comes to skill, I'm the one who reigns supreme!\\nI'll leave you in the dust like a used tissue, and then some!\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "# Callbacks support token-wise streaming\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/fatemehbalaei/Downloads/llama-2-7b-chat.Q4_0.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "question = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " everybody knows the answer is 4. right?\n",
      "\n",
      "Well, actually, it depends on who you ask! In mathematics, the answer to 2 + 2 can be whatever number you want it to be, as long as you define your operation correctly.\n",
      "\n",
      "In standard arithmetic, the answer to 2 + 2 is indeed 4. But in some non-standard models of arithmetic, the answer might be different. For example, in modular arithmetic, where the set of numbers is wrapped around a certain range (say, from 0 to 9), then 2 + 2 = 6.\n",
      "\n",
      "In any case, the answer to 2 + 2 depends on the context and how you define your operation. In mathematics, we often take it for granted that there is a single \"correct\" answer to an arithmetic problem, but this is not always the case!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6658.54 ms\n",
      "llama_print_timings:      sample time =      45.48 ms /   185 runs   (    0.25 ms per token,  4068.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7832.88 ms /     7 tokens ( 1118.98 ms per token,     0.89 tokens per second)\n",
      "llama_print_timings:        eval time = 1551144.70 ms /   184 runs   ( 8430.13 ms per token,     0.12 tokens per second)\n",
      "llama_print_timings:       total time = 1561360.66 ms /   191 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n everybody knows the answer is 4. right?\\n\\nWell, actually, it depends on who you ask! In mathematics, the answer to 2 + 2 can be whatever number you want it to be, as long as you define your operation correctly.\\n\\nIn standard arithmetic, the answer to 2 + 2 is indeed 4. But in some non-standard models of arithmetic, the answer might be different. For example, in modular arithmetic, where the set of numbers is wrapped around a certain range (say, from 0 to 9), then 2 + 2 = 6.\\n\\nIn any case, the answer to 2 + 2 depends on the context and how you define your operation. In mathematics, we often take it for granted that there is a single \"correct\" answer to an arithmetic problem, but this is not always the case!'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = llm.invoke(\"2 + 2 = ?\")\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: To add two decimals, you need to carry out the addition just like with whole numbers.\n",
      "\n",
      "2.009 + 5.3399 = 7.3499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6658.54 ms\n",
      "llama_print_timings:      sample time =       6.42 ms /    48 runs   (    0.13 ms per token,  7475.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16551.34 ms /    14 tokens ( 1182.24 ms per token,     0.85 tokens per second)\n",
      "llama_print_timings:        eval time =  360441.83 ms /    47 runs   ( 7668.98 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time =  377449.88 ms /    61 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nAnswer: To add two decimals, you need to carry out the addition just like with whole numbers.\\n\\n2.009 + 5.3399 = 7.3499'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = llm.invoke(\"2.009 + 5.3399 = ?\")\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/fatemehbalaei/Downloads/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reminder id x-apple-reminder://E3303A80-3210-46D3-A40F-72644574AB86\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from langchain_community.llms import LlamaCpp  # Ensure this import matches your environment\n",
    "\n",
    "# Your custom function\n",
    "def add_reminder_mac(reminder_text, list_name=\"Reminders\"):\n",
    "    applescript_command = f'''osascript -e 'tell application \"Reminders\" to make new reminder with properties {{name:\"{reminder_text}\", body:\"Generated by my model\", remind me date:(current date)}}' '''\n",
    "    subprocess.run(applescript_command, shell=True)\n",
    "\n",
    "# Setup for LlamaCpp model (placeholder setup)\n",
    "callback_manager = None  # Define your callback manager appropriately\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/fatemehbalaei/Downloads/llama-2-7b-chat.Q4_0.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Function to handle the model's output\n",
    "def process_model_output(output):\n",
    "    # Check if the output includes a reminder\n",
    "    if \"Reminder:\" in output:\n",
    "        # Extract the reminder text\n",
    "        reminder_text = output.split(\"Reminder:\",1)[1].strip()\n",
    "        # Call the function to add a reminder\n",
    "        add_reminder_mac(reminder_text)\n",
    "\n",
    "# Example of invoking the model and processing its output\n",
    "# Replace this with the actual way you get output from your model\n",
    "model_output = \"Reminder: set a reminder to make the presentation at 12:30\"\n",
    "process_model_output(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calendar_event_mac(event_title, start_date, end_date, calendar_name=\"Calendar\"):\n",
    "    applescript_command = f'''osascript -e 'tell application \"Calendar\"\n",
    "        tell calendar \"{calendar_name}\"\n",
    "            make new event with properties {{summary:\"{event_title}\", start date:date \"{start_date}\", end date:date \"{end_date}\"}}\n",
    "        end tell\n",
    "    end tell' '''\n",
    "    subprocess.run(applescript_command, shell=True)\n",
    "def perform_basic_calculation(expression):\n",
    "    try:\n",
    "        # Safely evaluate the basic arithmetic expression\n",
    "        result = eval(expression, {\"__builtins__\": None}, {\"+\": operator.add, \"-\": operator.sub, \"*\": operator.mul, \"/\": operator.div})\n",
    "        print(f\"Calculation result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing calculation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/fatemehbalaei/Downloads/codellama-7b.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-7b-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = codellama_codellama-7b-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.95 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '16384', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'codellama_codellama-7b-hf'}\n",
      "Using fallback chat format: None\n",
      "269:269: syntax error: Expected end of line but found end of script. (-2741)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import operator\n",
    "import re\n",
    "import ast\n",
    "from langchain_community.llms import LlamaCpp  # Ensure this import matches your environment\n",
    "\n",
    "# Custom function to add a reminder\n",
    "def add_reminder_mac(reminder_text, list_name=\"Reminders\"):\n",
    "    applescript_command = f'''osascript -e 'tell application \"Reminders\" to make new reminder with properties {{name:\"{reminder_text}\", body:\"Generated by my model\", remind me date:(current date)}}' '''\n",
    "    subprocess.run(applescript_command, shell=True)\n",
    "\n",
    "# Custom function to perform basic calculations\n",
    "def perform_calculation(expression):\n",
    "    try:\n",
    "        # Only allow basic arithmetic operations for safety\n",
    "        result = eval(expression, {\"__builtins__\": None}, operators)\n",
    "        print(f\"Calculation result: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing calculation: {e}\")\n",
    "\n",
    "# Custom function to add an event to the macOS Calendar\n",
    "def add_calendar_event_mac(event_title, start_date, end_date, calendar_name=\"Calendar\"):\n",
    "    applescript_command = f'''osascript -e 'tell application \"Calendar\"\n",
    "        tell application \"Calendar\"\n",
    "            tell calendar \"Work\"\n",
    "                make new event with properties {{summary:\"{event_title}\", start date:date \"{start_date}\", end date:date \"{end_date}\"}} at end of calendar\n",
    "        end tell\n",
    "    end tell' '''\n",
    "    subprocess.run(applescript_command, shell=True)\n",
    "\n",
    "# Operators allowed in calculations for safety\n",
    "operators = {ast.Add: operator.add, ast.Sub: operator.sub, ast.Mult: operator.mul, ast.Div: operator.truediv}\n",
    "\n",
    "# Setup for LlamaCpp model (placeholder setup)\n",
    "callback_manager = None  # Define your callback manager appropriately\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/fatemehbalaei/Downloads/codellama-7b.Q4_0.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Function to handle the model's output\n",
    "def process_model_output(output):\n",
    "    # Check if the output includes a reminder\n",
    "    if \"Reminder:\" in output:\n",
    "        reminder_text = output.split(\"Reminder:\", 1)[1].strip()\n",
    "        add_reminder_mac(reminder_text)\n",
    "    # Check for a calculation request\n",
    "    elif \"Calculate:\" in output:\n",
    "        expression = output.split(\"Calculate:\", 1)[1].strip()\n",
    "        perform_calculation(expression)\n",
    "    # Check for an event addition request\n",
    "    elif \"Event:\" in output:\n",
    "        # This is simplified; in a real scenario, you'd extract specific details\n",
    "        event_details = output.split(\"Event:\", 1)[1].strip()\n",
    "        add_calendar_event_mac(\"Event\", \"Start Date\", \"End Date\")  # Placeholder for demonstration\n",
    "\n",
    "# Example of invoking the model and processing its output\n",
    "# model_output = \"Reminder: Check the new model outputs\"start date:date \"{start_date}\", end date:date \"{end_date}\"\n",
    "model_output = \"Event: start date: Tuesday, April 18, 2024 10:00 AM, end date:Tuesday, April 18, 2024 11:00 AM\"\n",
    "process_model_output(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task': 'calculator', 'input': 'What is 50 / 76?', 'output': 'The answer is 0.6578947368421053.'}, {'task': 'calculator', 'input': 'What is 36 + 78.79?', 'output': 'The answer is 114.79.'}, {'task': 'calculator', 'input': 'What is 24.22 + 84.06?', 'output': 'The answer is 108.28.'}, {'task': 'calculator', 'input': 'What is 20.78 * 53?', 'output': 'The answer is 1101.3400000000001.'}, {'task': 'calculator', 'input': 'What is 66 + 47.37?', 'output': 'The answer is 113.37.'}]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import operator\n",
    "\n",
    "ops = { '+': operator.add, '-': operator.sub, '*': operator.mul, '/': operator.truediv }\n",
    "data = []\n",
    "\n",
    "def generate_number():\n",
    "    if random.choice([True, False]):  # Randomly decide between int and float\n",
    "        return random.randint(1, 100)  # Return an int\n",
    "    else:\n",
    "        # Return a float with 2 decimal places\n",
    "        return round(random.uniform(1, 100), 2)\n",
    "\n",
    "for _ in range(1000):  # Generate 1000 examples\n",
    "    op = random.choice(list(ops.keys()))\n",
    "    nums = [generate_number() for _ in range(2)]\n",
    "    if op == '/':  # Modify for division\n",
    "        nums[1] = generate_number() if nums[1] != 0 else generate_number() + 1  # Ensure divisor isn't 0\n",
    "    question = f\"What is {nums[0]} {op} {nums[1]}?\"\n",
    "    answer = ops[op](nums[0], nums[1])\n",
    "    data.append({\n",
    "        \"task\": \"calculator\",\n",
    "        \"input\": question,\n",
    "        \"output\": f\"The answer is {answer}.\"  # Display the answer directly\n",
    "    })\n",
    "\n",
    "# Print the first 5 to check\n",
    "print(data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder Templates\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "reminder_templates = [\n",
    "    \"Remind me to {task} on {date}.\",\n",
    "    \"Set a reminder for {task} at {time}.\"\n",
    "]\n",
    "\n",
    "# Calendar Event Templates\n",
    "calendar_templates = [\n",
    "    \"Schedule a meeting about {topic} on {date} at {time}.\",\n",
    "    \"Add an event: {event} on {date} at {time}.\"\n",
    "]\n",
    "\n",
    "# Tasks and Topics for filling the templates\n",
    "tasks = [\"call John\", \"visit the dentist\", \"attend the webinar\", \"check emails\", \"water the plants\"]\n",
    "topics = [\"project update\", \"strategy planning\", \"budget review\", \"team building activity\"]\n",
    "events = [\"doctor's appointment\", \"dinner with friends\", \"team meeting\", \"client presentation\"]\n",
    "\n",
    "# Function to generate a random date\n",
    "def random_future_date():\n",
    "    today = datetime.date.today()\n",
    "    future_date = today + datetime.timedelta(days=random.randint(1, 365))\n",
    "    return future_date.strftime(\"%B %d, %Y\")  # Format e.g., \"March 10, 2024\"\n",
    "\n",
    "# Function to generate a random time\n",
    "def random_time():\n",
    "    hour = random.randint(8, 17)  # Assuming between 8 AM and 5 PM\n",
    "    minute = random.choice([\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"01\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\"])\n",
    "    return f\"{hour}:{minute}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task': 'reminder', 'input': 'Remind me to visit the dentist on December 19, 2024.', 'output': 'Reminder set: Remind me to visit the dentist on December 19, 2024.'}, {'task': 'calendar', 'input': \"Add an event: doctor's appointment on January 20, 2025 at 14:08.\", 'output': \"Event scheduled: Add an event: doctor's appointment on January 20, 2025 at 14:08.\"}, {'task': 'reminder', 'input': 'Set a reminder for call John at 11:26.', 'output': 'Reminder set: Set a reminder for call John at 11:26.'}, {'task': 'calculator', 'input': 'What is 32.6 / 56?', 'output': 'The answer is 0.5821428571428572.'}, {'task': 'reminder', 'input': 'Set a reminder for check emails at 10:30.', 'output': 'Reminder set: Set a reminder for check emails at 10:30.'}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset list\n",
    "# dataset = []\n",
    "\n",
    "# Generate reminder records\n",
    "for _ in range(1000):  # 500 examples of each type\n",
    "    template = random.choice(reminder_templates)\n",
    "    task = random.choice(tasks)\n",
    "    record = template.format(task=task, date=random_future_date(), time=random_time())\n",
    "    data.append({\"task\": \"reminder\", \"input\": record, \"output\": \"Reminder set: \" + record})\n",
    "\n",
    "# Generate calendar event records\n",
    "for _ in range(1000):\n",
    "    template = random.choice(calendar_templates)\n",
    "    topic_event = random.choice(topics + events)  # Combine topics and events for diversity\n",
    "    record = template.format(topic=topic_event, event=topic_event, date=random_future_date(), time=random_time())\n",
    "    data.append({\"task\": \"calendar\", \"input\": record, \"output\": \"Event scheduled: \" + record})\n",
    "\n",
    "# Optionally, shuffle the dataset to mix reminder and calendar examples\n",
    "random.shuffle(data)\n",
    "\n",
    "# Show a sample of the dataset\n",
    "print(data[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-cpp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
